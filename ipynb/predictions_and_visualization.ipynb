{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import pylab as plt\n",
    "from munch import munchify\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "# from preprocessing.s02_create_dataset import load_cores_h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project root\n",
    "project_root = 'C:/Users/Mahdi/Desktop/Summer21/RA/Codes/Minh_Mahdi_mod/prostate_cancer_classification'\n",
    "\n",
    "args = {}\n",
    "\n",
    "# yaml configuration file location\n",
    "args['config'] = '../yamls/coteaching_local_inference_Exact2D.yml'\n",
    "# experiment location to load\n",
    "args['exp_suffix'] = '_Patch/lr1e-5_fr.4numgrad6----res10_UVA400_testiLR_crrctep11_2'\n",
    "\n",
    "# opt is a dictionary which contains all configurations\n",
    "with open(args['config']) as f:\n",
    "    opt = yaml.load(f, Loader)\n",
    "opt.update(args)\n",
    "opt = munchify(opt)\n",
    "opt.project_root = project_root\n",
    "opt = setup_directories(opt)\n",
    "\n",
    "num_workers = 0\n",
    "device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the neural network\n",
    "net = construct_network(device, opt)\n",
    "# if len(net) > 1:\n",
    "#     net = net[0]()\n",
    "#     suffix = '_1'\n",
    "# else:\n",
    "net = net[0]()\n",
    "suffix = '' if 'ct' not in args['exp_suffix'] else '_1'\n",
    "# loading the saved weights to it\n",
    "net.load_state_dict(torch.load(f'{opt.project_root}/{opt.paths.checkpoint_dir}/{opt.test.which_iter}_coreN{suffix}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import DatasetV1, extract_subset\n",
    "\n",
    "initial_min_inv=.8\n",
    "min_inv=.4\n",
    "\n",
    "input_data = load_pickle('../datasets/BK_RF_P1_140_balance__20210203-175808_mimic.pkl')\n",
    "\n",
    "trn_ds = DatasetV1(*extract_subset(input_data, 'train', min_inv), aug_type='none',\n",
    "                   initial_min_inv=initial_min_inv, transform_prob=.2, degree=1, n_neighbor=0)\n",
    "\n",
    "# transformer = robust_norm(np.concatenate(trn_ds.data))[1]\n",
    "transformer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../metadata/matched_tmi_cores_idx.pkl', 'rb') as fp:\n",
    "    core_indices = pickle.load(fp)\n",
    "tmp = core_indices['train']\n",
    "for set_name in ['val', 'test']:\n",
    "    tmp.update(core_indices[set_name])\n",
    "\n",
    "# Re-split dataset\n",
    "core_indices = {}\n",
    "for set_name in ['train', 'val', 'test']:\n",
    "    core_indices[set_name] = {}\n",
    "    for pid in np.unique(input_data[f'PatientId_{set_name}']):\n",
    "        core_indices[set_name][pid] = tmp[pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?patient/s]\n",
      "  0%|          | 0/7 [00:00<?, ?batch/s]\u001B[A\n",
      " 14%|█▍        | 1/7 [00:00<00:03,  1.76batch/s]\u001B[A\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.61batch/s]\u001B[A\n",
      "/home/minh/PycharmProjects/prostate_cancer_classification_v1/utils/cores.py:224: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "  6%|▌         | 1/18 [01:08<19:32, 68.99s/patient]\n",
      "  0%|          | 0/3 [00:00<?, ?batch/s]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:00<00:01,  1.44batch/s]\u001B[A\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.94batch/s]\u001B[A\n",
      " 11%|█         | 2/18 [01:45<13:15, 49.69s/patient]\n",
      "  0%|          | 0/13 [00:00<?, ?batch/s]\u001B[A\n",
      "  8%|▊         | 1/13 [00:00<00:08,  1.45batch/s]\u001B[A\n",
      " 46%|████▌     | 6/13 [00:00<00:00,  9.57batch/s]\u001B[A\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.74batch/s]\u001B[A\n",
      " 17%|█▋        | 3/18 [04:12<23:30, 94.06s/patient]\n",
      "  0%|          | 0/8 [00:00<?, ?batch/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▎        | 1/8 [00:00<00:05,  1.38batch/s]\u001B[A\n",
      "100%|██████████| 8/8 [00:00<00:00,  8.14batch/s]\u001B[A\n",
      " 28%|██▊       | 5/18 [06:14<16:19, 75.35s/patient]\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]\u001B[A\n",
      " 10%|█         | 1/10 [00:00<00:06,  1.44batch/s]\u001B[A\n",
      " 60%|██████    | 6/10 [00:00<00:00,  9.60batch/s]\u001B[A\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.03batch/s]\u001B[A\n",
      " 33%|███▎      | 6/18 [07:51<16:20, 81.70s/patient]\n",
      "  0%|          | 0/7 [00:00<?, ?batch/s]\u001B[A\n",
      " 14%|█▍        | 1/7 [00:00<00:04,  1.41batch/s]\u001B[A\n",
      "100%|██████████| 7/7 [00:01<00:00,  6.92batch/s]\u001B[A\n",
      " 39%|███▉      | 7/18 [09:17<15:12, 82.99s/patient]\n",
      "  0%|          | 0/8 [00:00<?, ?batch/s]\u001B[A\n",
      " 12%|█▎        | 1/8 [00:00<00:04,  1.49batch/s]\u001B[A\n",
      "100%|██████████| 8/8 [00:01<00:00,  7.48batch/s]\u001B[A\n",
      " 44%|████▍     | 8/18 [10:54<14:31, 87.15s/patient]\n",
      "  0%|          | 0/3 [00:00<?, ?batch/s]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:00<00:01,  1.47batch/s]\u001B[A\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.39batch/s]\u001B[A\n",
      " 50%|█████     | 9/18 [11:20<10:19, 68.87s/patient]\n",
      "  0%|          | 0/2 [00:00<?, ?batch/s]\u001B[A\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.42batch/s]\u001B[A\n",
      " 56%|█████▌    | 10/18 [11:38<07:10, 53.81s/patient]\n",
      "  0%|          | 0/7 [00:00<?, ?batch/s]\u001B[A\n",
      " 14%|█▍        | 1/7 [00:00<00:03,  1.52batch/s]\u001B[A\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.66batch/s]\u001B[A\n",
      " 61%|██████    | 11/18 [12:51<06:57, 59.62s/patient]\n",
      "  0%|          | 0/9 [00:00<?, ?batch/s]\u001B[A\n",
      " 11%|█         | 1/9 [00:00<00:05,  1.35batch/s]\u001B[A\n",
      " 67%|██████▋   | 6/9 [00:00<00:00,  9.11batch/s]\u001B[A\n",
      "100%|██████████| 9/9 [00:01<00:00,  8.55batch/s]\u001B[A\n",
      " 67%|██████▋   | 12/18 [15:09<08:18, 83.10s/patient]\n",
      "  0%|          | 0/17 [00:00<?, ?batch/s]\u001B[A\n",
      "  6%|▌         | 1/17 [00:00<00:12,  1.31batch/s]\u001B[A\n",
      " 29%|██▉       | 5/17 [00:00<00:01,  7.25batch/s]\u001B[A\n",
      " 59%|█████▉    | 10/17 [00:00<00:00, 14.80batch/s]\u001B[A\n",
      "100%|██████████| 17/17 [00:01<00:00, 10.67batch/s]\u001B[A\n",
      " 72%|███████▏  | 13/18 [18:39<10:05, 121.09s/patient]\n",
      "  0%|          | 0/5 [00:00<?, ?batch/s]\u001B[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.50batch/s]\u001B[A\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.71batch/s]\u001B[A\n",
      " 78%|███████▊  | 14/18 [19:33<06:43, 100.93s/patient]\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]\u001B[A\n",
      " 10%|█         | 1/10 [00:00<00:06,  1.48batch/s]\u001B[A\n",
      " 60%|██████    | 6/10 [00:00<00:00,  9.78batch/s]\u001B[A\n",
      "100%|██████████| 10/10 [00:01<00:00,  7.15batch/s]\u001B[A\n",
      " 83%|████████▎ | 15/18 [21:45<05:30, 110.21s/patient]\n",
      "  0%|          | 0/4 [00:00<?, ?batch/s]\u001B[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.51batch/s]\u001B[A\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.13batch/s]\u001B[A\n",
      " 89%|████████▉ | 16/18 [22:36<03:04, 92.39s/patient] \n",
      "  0%|          | 0/8 [00:00<?, ?batch/s]\u001B[A\n",
      " 12%|█▎        | 1/8 [00:00<00:04,  1.53batch/s]\u001B[A\n",
      "100%|██████████| 8/8 [00:00<00:00,  8.05batch/s]\u001B[A\n",
      " 94%|█████████▍| 17/18 [24:18<01:35, 95.40s/patient]\n",
      "  0%|          | 0/15 [00:00<?, ?batch/s]\u001B[A\n",
      "  7%|▋         | 1/15 [00:00<00:10,  1.35batch/s]\u001B[A\n",
      " 40%|████      | 6/15 [00:00<00:01,  8.96batch/s]\u001B[A\n",
      " 73%|███████▎  | 11/15 [00:00<00:00, 16.03batch/s]\u001B[A\n",
      "100%|██████████| 15/15 [00:01<00:00, 12.18batch/s]\u001B[A\n",
      "100%|██████████| 18/18 [27:03<00:00, 90.17s/patient] \n",
      "  0%|          | 0/29 [00:00<?, ?patient/s]\n",
      "  0%|          | 0/5 [00:00<?, ?batch/s]\u001B[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.54batch/s]\u001B[A\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.16batch/s]\u001B[A\n",
      "  3%|▎         | 1/29 [00:49<23:03, 49.41s/patient]\n",
      "  0%|          | 0/2 [00:00<?, ?batch/s]\u001B[A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  1.52batch/s]\u001B[A\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.29batch/s]\u001B[A\n",
      "  7%|▋         | 2/29 [01:12<15:23, 34.19s/patient]\n",
      "  0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.29batch/s]\u001B[A\n",
      " 10%|█         | 3/29 [01:39<13:12, 30.49s/patient]\n",
      "  0%|          | 0/4 [00:00<?, ?batch/s]\u001B[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.56batch/s]\u001B[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.04batch/s]\u001B[A\n",
      " 14%|█▍        | 4/29 [02:29<16:02, 38.50s/patient]\n",
      "  0%|          | 0/7 [00:00<?, ?batch/s]\u001B[A\n",
      " 14%|█▍        | 1/7 [00:00<00:03,  1.55batch/s]\u001B[A\n",
      "100%|██████████| 7/7 [00:01<00:00,  5.30batch/s]\u001B[A\n",
      " 17%|█▋        | 5/29 [04:10<24:21, 60.88s/patient]\n",
      "  0%|          | 0/3 [00:00<?, ?batch/s]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:00<00:01,  1.63batch/s]\u001B[A\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.28batch/s]\u001B[A\n",
      " 21%|██        | 6/29 [05:02<22:12, 57.93s/patient]\n",
      "  0%|          | 0/5 [00:00<?, ?batch/s]\u001B[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.34batch/s]\u001B[A\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.55batch/s]\u001B[A\n",
      " 24%|██▍       | 7/29 [05:53<20:27, 55.79s/patient]\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]\u001B[A\n",
      " 10%|█         | 1/10 [00:00<00:06,  1.34batch/s]\u001B[A\n",
      " 40%|████      | 4/10 [00:00<00:01,  5.50batch/s]\u001B[A\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.71batch/s][A\n",
      " 28%|██▊       | 8/29 [07:26<23:36, 67.47s/patient]\n",
      "  0%|          | 0/4 [00:00<?, ?batch/s]\u001B[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.61batch/s]\u001B[A\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.02batch/s]\u001B[A\n",
      " 31%|███       | 9/29 [08:21<21:13, 63.69s/patient]\n",
      "  0%|          | 0/8 [00:00<?, ?batch/s]\u001B[A\n",
      " 12%|█▎        | 1/8 [00:00<00:04,  1.61batch/s]\u001B[A\n",
      "100%|██████████| 8/8 [00:00<00:00,  8.02batch/s]\u001B[A\n",
      " 34%|███▍      | 10/29 [09:40<21:36, 68.25s/patient]\n",
      "  0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38batch/s]\u001B[A\n",
      " 38%|███▊      | 11/29 [10:06<16:38, 55.48s/patient]\n",
      "  0%|          | 0/2 [00:00<?, ?batch/s]\u001B[A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  1.59batch/s]\u001B[A\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.32batch/s]\u001B[A\n",
      " 41%|████▏     | 12/29 [10:31<13:02, 46.03s/patient]\n",
      "  0%|          | 0/6 [00:00<?, ?batch/s]\u001B[A\n",
      " 17%|█▋        | 1/6 [00:00<00:03,  1.35batch/s]\u001B[A\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.11batch/s]\u001B[A\n",
      " 45%|████▍     | 13/29 [11:49<14:54, 55.93s/patient]\n",
      "  0%|          | 0/2 [00:00<?, ?batch/s]\u001B[A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  1.61batch/s]\u001B[A\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.43batch/s]\u001B[A\n",
      " 48%|████▊     | 14/29 [12:09<11:14, 44.99s/patient]\n",
      "  0%|          | 0/4 [00:00<?, ?batch/s]\u001B[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.62batch/s]\u001B[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.47batch/s]\u001B[A\n",
      " 52%|█████▏    | 15/29 [12:47<09:59, 42.86s/patient]\n",
      "  0%|          | 0/6 [00:00<?, ?batch/s]\u001B[A\n",
      " 17%|█▋        | 1/6 [00:00<00:03,  1.60batch/s]\u001B[A\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.36batch/s]\u001B[A\n",
      " 55%|█████▌    | 16/29 [13:42<10:02, 46.35s/patient]\n",
      "  0%|          | 0/4 [00:00<?, ?batch/s]\u001B[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.58batch/s]\u001B[A\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.86batch/s]\u001B[A\n",
      " 59%|█████▊    | 17/29 [14:34<09:39, 48.30s/patient]\n",
      "  0%|          | 0/2 [00:00<?, ?batch/s]\u001B[A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  1.61batch/s]\u001B[A\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.49batch/s]\u001B[A\n",
      " 62%|██████▏   | 18/29 [14:54<07:16, 39.67s/patient]\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]\u001B[A\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.55batch/s]\u001B[A\n",
      " 60%|██████    | 6/10 [00:00<00:00, 10.14batch/s]\u001B[A\n",
      "100%|██████████| 10/10 [00:00<00:00, 10.01batch/s]\u001B[A\n",
      " 66%|██████▌   | 19/29 [16:39<09:53, 59.36s/patient]\n",
      "  0%|          | 0/5 [00:00<?, ?batch/s]\u001B[A\n",
      " 20%|██        | 1/5 [00:00<00:03,  1.31batch/s]\u001B[A\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.94batch/s]\u001B[A\n",
      " 69%|██████▉   | 20/29 [17:34<08:41, 57.89s/patient]\n",
      "  0%|          | 0/5 [00:00<?, ?batch/s]\u001B[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.64batch/s]\u001B[A\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.85batch/s]\u001B[A\n",
      " 72%|███████▏  | 21/29 [18:25<07:28, 56.04s/patient]\n",
      "  0%|          | 0/5 [00:00<?, ?batch/s]\u001B[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.67batch/s]\u001B[A\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.23batch/s]\u001B[A\n",
      " 76%|███████▌  | 22/29 [19:19<06:27, 55.35s/patient]\n",
      "  0%|          | 0/9 [00:00<?, ?batch/s]\u001B[A\n",
      " 11%|█         | 1/9 [00:00<00:04,  1.62batch/s]\u001B[A\n",
      " 67%|██████▋   | 6/9 [00:00<00:00, 10.55batch/s]\u001B[A\n",
      "100%|██████████| 9/9 [00:00<00:00,  9.37batch/s]\u001B[A\n",
      " 79%|███████▉  | 23/29 [21:03<06:59, 69.99s/patient]\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]\u001B[A\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.61batch/s]\u001B[A\n",
      " 60%|██████    | 6/10 [00:00<00:00, 10.50batch/s]\u001B[A\n",
      "100%|██████████| 10/10 [00:00<00:00, 10.73batch/s]\u001B[A\n",
      " 83%|████████▎ | 24/29 [22:47<06:40, 80.09s/patient]\n",
      "  0%|          | 0/21 [00:00<?, ?batch/s]\u001B[A\n",
      "  5%|▍         | 1/21 [00:00<00:14,  1.40batch/s]\u001B[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6/21 [00:00<00:01,  9.33batch/s]\u001B[A\n",
      " 52%|█████▏    | 11/21 [00:00<00:00, 16.75batch/s]\u001B[A\n",
      " 76%|███████▌  | 16/21 [00:01<00:00, 23.49batch/s]\u001B[A\n",
      "100%|██████████| 21/21 [00:01<00:00, 15.20batch/s]\u001B[A\n",
      " 86%|████████▌ | 25/29 [26:40<08:23, 125.88s/patient]\n",
      "  0%|          | 0/7 [00:00<?, ?batch/s]\u001B[A\n",
      " 14%|█▍        | 1/7 [00:00<00:03,  1.61batch/s]\u001B[A\n",
      "100%|██████████| 7/7 [00:01<00:00,  7.00batch/s]\u001B[A\n",
      " 90%|████████▉ | 26/29 [27:52<05:29, 109.87s/patient]\n",
      "  0%|          | 0/3 [00:00<?, ?batch/s]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:00<00:01,  1.71batch/s]\u001B[A\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.84batch/s]\u001B[A\n",
      " 93%|█████████▎| 27/29 [28:19<02:49, 84.89s/patient] \n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]\u001B[A\n",
      " 10%|█         | 1/10 [00:00<00:07,  1.18batch/s]\u001B[A\n",
      " 60%|██████    | 6/10 [00:00<00:00,  8.09batch/s]\u001B[A\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.00batch/s]\u001B[A\n",
      " 97%|█████████▋| 28/29 [30:35<01:40, 100.39s/patient]\n",
      "  0%|          | 0/2 [00:00<?, ?batch/s]\u001B[A\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.63batch/s]\u001B[A\n",
      "100%|██████████| 29/29 [30:53<00:00, 63.92s/patient] \n"
     ]
    }
   ],
   "source": [
    "from skimage.morphology import remove_small_objects\n",
    "\n",
    "def predict_and_visualize(patient_id, transformer=None):\n",
    "    _cores = load_cores_h5py(patient_id, core_indices[state][patient_id])\n",
    "\n",
    "    inputs = []\n",
    "    for core in _cores:\n",
    "        if (core.roi[0] == 1).sum() == 0:\n",
    "            print(core.core_id)\n",
    "            continue\n",
    "        inputs.append(core.rf[:, core.roi[0] == 1].T[:, np.newaxis])\n",
    "\n",
    "        core.wp[0] = remove_small_objects(core.wp[0].astype('bool'))\n",
    "\n",
    "    if len(inputs) == 0:\n",
    "        print(patient_id)\n",
    "        return\n",
    "\n",
    "\n",
    "    # Normalization & Concatenation\n",
    "    signal_test = np.concatenate(inputs, axis=0)\n",
    "#     signal_test = robust_norm(signal_test, transformer)[0]\n",
    "\n",
    "    # Tensor dataset\n",
    "    dataset = TensorDataset(torch.tensor(signal_test, dtype=torch.float32))\n",
    "    dataloader = DataLoader(dataset, shuffle=False, num_workers=num_workers, batch_size=opt.test.batch_size)\n",
    "\n",
    "    outputs = []\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader, unit=\"batch\") as t_epoch:\n",
    "            for i, (data, ) in enumerate(t_epoch):\n",
    "                output = net(data.cuda())\n",
    "                outputs.append(F.softmax(output, dim=1).cpu().detach().numpy())\n",
    "\n",
    "    outputs = np.concatenate(outputs)\n",
    "\n",
    "    current_idx = 0\n",
    "    for i, core in enumerate(_cores):\n",
    "        heatmap = np.zeros_like(core.roi, dtype='float32')\n",
    "        core_len = int(core.roi.sum())\n",
    "        heatmap[:, core.roi[0] == 1] = outputs[current_idx: current_idx + core_len, 1]\n",
    "        core.heatmap = heatmap\n",
    "        current_idx += core_len\n",
    "\n",
    "    _cores = [rf2bm_wrapper(core, quick_convert=True) for core in _cores]\n",
    "\n",
    "#     figure_filename = '/'.join((figure_dir, f'Patient{_cores[0].patient_id}.png'))\n",
    "    fig = review_cores(_cores, figure_dir=figure_dir, patient_id=_cores[0].patient_id)\n",
    "#     fig.savefig(figure_filename, bbox_inches='tight')\n",
    "#     plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for state in ['test']:\n",
    "#     figure_dir = '/'.join((opt.project_root, opt.paths.result_dir.replace('results', 'figures') + '/' + state))\n",
    "#     os.makedirs(figure_dir, exist_ok=True)\n",
    "#     patient_ids = core_indices[state].keys()\n",
    "# #     patient_ids\n",
    "\n",
    "#     with tqdm(patient_ids, unit=\"patient\") as t_patient:\n",
    "#         for i, patient_id in enumerate(t_patient):\n",
    "#             predict_and_visualize(patient_id, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def predict(net, tst_dl, device):\n",
    "    outputs = []\n",
    "    entropic_scores = []\n",
    "    features = []\n",
    "    total = correct = 0\n",
    "    inputs = []\n",
    "    net.eval()\n",
    "\n",
    "    # apply model on test signals\n",
    "    for batch in tst_dl:\n",
    "        x_raw, y_batch, n_batch, _ = [t.to(device) for t in batch]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = net(x_raw, n_batch)\n",
    "            pred = F.softmax(pred, dim=1)\n",
    "\n",
    "            probabilities = pred\n",
    "            entropies = -(probabilities * torch.log(probabilities)).sum(dim=1)\n",
    "            entropic_scores.append((-entropies).cpu().numpy())\n",
    "\n",
    "            inputs.append(x_raw.cpu().numpy())\n",
    "            outputs.append(pred.cpu().numpy())\n",
    "            total += y_batch.size(0)\n",
    "            correct += (pred.argmax(dim=1) == torch.argmax(y_batch, dim=1)).sum().item()\n",
    "\n",
    "    inputs = np.concatenate(inputs)\n",
    "    outputs = np.concatenate(outputs)\n",
    "    entropic_scores = np.concatenate(entropic_scores)\n",
    "\n",
    "    return inputs, outputs, entropic_scores, features, correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.dataset import create_datasets_test\n",
    "\n",
    "test_set = create_datasets_test(None, min_inv=min_inv, state='test', norm=False, input_data=input_data,\n",
    "                                transformer=None)\n",
    "\n",
    "test_set[0] = create_loaders_test(test_set[0], bs=4096, jobs=12)[0]\n",
    "data_loader, core_len, true_involvement, patient_id_bk, gs_bk, roi_coors, ts_id, c_id = test_set\n",
    "\n",
    "# Evaluation\n",
    "inputs, predictions, ood, latents, acc_s = predict(net, data_loader, device)\n",
    "\n",
    "# Infer core-wise predictions\n",
    "inputs, predicted_involvement, ood, latents, prediction_maps = infer_core_wise(inputs, predictions, core_len,\n",
    "                                                                               roi_coors, ood,\n",
    "                                                                               latents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = {'acc_s': acc_s}\n",
    "import matplotlib\n",
    "scores = compute_metrics(predicted_involvement, true_involvement, declare_thr=opt.declare_thr,\n",
    "                         current_epoch=0, verbose=True, scores=scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def norm_01(x):\n",
    "    return (x - x.min())/ (x.max() - x.min())\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "declare_thr = .4\n",
    "fig2 = plt.figure(2)\n",
    "\n",
    "predicted_involvement = np.array(predicted_involvement)\n",
    "idx_b = np.array(true_involvement) > 0\n",
    "idx_c = np.array(true_involvement) == 0\n",
    "ax2 = sns.scatterplot(x=true_involvement[idx_b],\n",
    "                      y=predicted_involvement[idx_b],\n",
    "                      legend=False, s=200, color='red')\n",
    "sns.scatterplot(x=true_involvement[idx_c],\n",
    "                y=predicted_involvement[idx_c],\n",
    "                legend=False, s=200, color='blue', ax=ax2)\n",
    "\n",
    "diag = np.arange(0, 1, .05)\n",
    "sns.lineplot(x=diag, y=diag, color='b', ax=ax2)\n",
    "# ax2.axvspan(-.1, 0.1, -.1, declare_thr+.015, alpha=.2, facecolor='lightgreen')\n",
    "# ax2.axvspan(-.1, 0.1, declare_thr + .015, 1., alpha=.2, facecolor='red')\n",
    "# ax2.axvspan(0.101, 1.1, -.1, declare_thr+.015, alpha=.2, facecolor='grey')\n",
    "\n",
    "ax2.axis('square')\n",
    "ax2.set(ylim=[-.05, 1.05], xlim=[-.05, 1.05])\n",
    "unit = 1e-3\n",
    "\n",
    "######################\n",
    "cmap_b = matplotlib.cm.get_cmap('Blues')\n",
    "cmap_b = np.array([cmap_b(_) for _ in np.arange(0, int(255*.45), unit)])\n",
    "tmp = []\n",
    "for i, v in enumerate(np.arange(1.05, -.05, -unit)):\n",
    "    tmp.append(cmap_b[i])\n",
    "    if i == 450:\n",
    "        break\n",
    "cmap_b = np.array(tmp)[::-1]\n",
    "\n",
    "cmap_c = matplotlib.cm.get_cmap('Reds')\n",
    "cmap_c = np.array([cmap_c(_) for _ in np.arange(0, int(255*.65), unit)])\n",
    "tmp = []\n",
    "for i, v in enumerate(np.arange(-.05, 1.05, unit)):\n",
    "    tmp.append(cmap_c[i])\n",
    "    if i == 650:\n",
    "        break\n",
    "cmap_c = np.array(tmp)[::-1]\n",
    "\n",
    "######################\n",
    "alpha = .2\n",
    "\n",
    "for i, v in enumerate(np.arange(-.05, 1.05, unit)):\n",
    "#     ax2.axhspan(v-.001, v+.001, .14, 1., alpha=.5, facecolor=cmap_c[i])  # 'moccasin'\n",
    "#     ax2.axhspan(v-.001, v+.001, -.05, .139, alpha=.5, facecolor=cmap_c[i])  # 'moccasin'\n",
    "    ax2.axhspan(v-unit, v+unit, -.05, 1.05, alpha=alpha, facecolor=cmap_b[i])  # 'moccasin'\n",
    "    if i == 450:\n",
    "        break\n",
    "\n",
    "# cmap_c = cmap_c[::-1]\n",
    "for i, v in enumerate(np.arange(1.05, -.05, -unit)):\n",
    "    ax2.axhspan(v-unit, v+unit, -.05, 1.05, alpha=alpha, facecolor=cmap_c[i])  # 'moccasin'\n",
    "    if i == 650:\n",
    "        break\n",
    "\n",
    "# ax2.axvspan(0.101, 1.1, declare_thr + .015, 1., alpha=.2, facecolor='moccasin')\n",
    "ax2.axvline(x=.101, linewidth=.6, linestyle='--', color='black')\n",
    "ax2.axhline(y=declare_thr + .001, linewidth=.6, linestyle='--', color='black')\n",
    "\n",
    "# if scores is not None:\n",
    "#     ax2.set(title=f'Correlation Coefficient = {scores[\"corr\"]:.3f} | MAE = {scores[\"mae\"]:.3f}',\n",
    "#             xlabel='True Involvement', ylabel='Predicted Involvement'\n",
    "#             )\n",
    "ax2.axis('off')\n",
    "plt.gcf().set_size_inches(11.7, 8.27)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}