project_root: /home/minh/PycharmProjects/prostate_cancer_classification_v1
tasks: [ 'cls', ]
tasks_num_class: [ 2, ]
input_channels: [ 1, ]
num_workers: 15
model_name: coteaching
#self_train: False
self_train: False
variational: False
multitask: False
is_eval: False

paths: # 'exp_name' will be replaced by actual experiment name
  log_dir: experiments/exp_name/logs
  result_dir: experiments/exp_name/results
  checkpoint_dir: experiments/exp_name/checkpoints
  self_train_checkpoint: 'ckpt2/inception/SelfTime/ProstateTeUS/magnitude_warp_time_warp/0.2_3C/0/backbone_best.tar'

data_source:
  dataset: ProstateTeUS
  data_root: datasets
  train_set: BK_RF_P1_140_balance__20210203-175808_mimic.pkl  # BK_RF_P1_140_balance__20201127-104536.mat
  unsup_set: BK_RF_P1_140_balance__20210203-175808_unsup.pkl  # BK_RF_P1_140_balance__20201127-104536.mat
  test_set: BK_RF_P1_140_balance__20210203-175808_mimic.pkl
  unlabelled_set: none # unlabelled/BK_RF_P1_140_balance__20210203-175808_unsup.npy

# Tunable (iteratively) hyper-parameters
seed: 0
n_epochs: 100
total_iters: 20000
#backbone: [inception, inception]  # SimConv4
backbone: [ resnet, resnet ]  # SimConv4
exp_name: coteaching
exp_suffix: ''
test_batch_size: 2048
train_batch_size: 2048
lr: 1e-4
normalize_input: none
temperature: .5
aug_type: [ 'magnitude_warp', 'time_warp', 'scaling', 'window_slice', 'window_warp' ]  # 'none'
unsup_aug_type: [ 'magnitude_warp', 'time_warp', 'window_slice', 'window_warp' ]  # 'none', 'scaling'

loss_name: 'gce'
min_inv: .4

train:
  loss_coefficients: [ 1., 0.01 ]
  n_views: 1
  lr_scheduler:
    patience: 999
    trials: 0
    epoch_decay_start: -1
  resume: False
  retrain_resume: False
  policy_iter: best
  which_iter: warmup
  init_method: equal
  hard_sampling: False
  log_interval: 1
  val_interval: 1
  switch_core_no: 0
  coteaching:
    num_gradual: 20
    exponent: 1
    forget_rate: .4
    use_plus: False
    relax: False

arch:
  # inception
  num_blocks: 3
  out_channels: 64
  # resnet
  mid_channels: 32
  num_positions: 8

tensorboard:
  flush_secs: 10
  filename_suffix: ''

test:
  test_interval: 1
  which_iter: best

abstention:
  learn_epochs: 0
  abst_rate: 0.1
  alpha_final: 1.
  alpha_init_factor: 64.
  pid_tunings: [ .1, .1, .05 ]
elr_alpha: 3
elr_beta: .7